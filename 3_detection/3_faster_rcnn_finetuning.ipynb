{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfPPQ6ztJhv4"
   },
   "source": [
    "# TorchVision Faster R-CNN Finetuning Tutorial\n",
    "\n",
    "Based on [TorchVision Object Detection Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
    "\n",
    "For this tutorial, we will be finetuning a pre-trained Faster R-CNN model in the [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip). It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset. To run the notebook, download the dataset, unzip it and move it into the `data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sd4jlGp2eLm"
   },
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "Specifically for the torchvision reference scripts to work, the dataset `__getitem__` should return a tuple `(image, target)`, with:\n",
    "\n",
    "* `image`: a PIL Image of size (H, W)\n",
    "* `target`: a dictionary containing the following fields\n",
    "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
    "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
    "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
    "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
    "\n",
    "One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has both classes, your labels tensor should look like [1,2].\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n",
    "\n",
    "**See [`lib.penn_fundan`](./lib/penn_fundan.py) for the dataset implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoAEkUgn4uEq"
   },
   "source": [
    "## Defining your model\n",
    "\n",
    "In this tutorial, we will be using [Faster R-CNN](https://arxiv.org/abs/1506.01497) with a ResNet-50 FPN backbone. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjNHjVMOyYlH"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WXLwePV5ieP"
   },
   "source": [
    "## Training and evaluation functions\n",
    "\n",
    "We will be using a modification from [torchvision reference scripts for training object detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) that are included in the torchvision repository. This includes code for the training loop, evaluation with COCO metrics and image transformation utilities, so we don't need to write all these things ourselves.\n",
    "\n",
    "The scripts are in the directory `lib/detection/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YFJGJxk6XEs"
   },
   "source": [
    "### Putting everything together\n",
    "\n",
    "We now have the dataset class, the models and the data transforms. Let's instantiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5dGaIezze3y",
    "outputId": "5cf908c4-a3c6-4c7b-b6f5-7633e8b43594"
   },
   "outputs": [],
   "source": [
    "import lib.detection.transforms as T\n",
    "from lib.penn_fundan import PennFudanDataset\n",
    "\n",
    "# Define data transforms for training batches\n",
    "train_tfm = T.Compose([\n",
    "    T.ToTensor(),  # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    T.RandomHorizontalFlip(0.5)  # randomly flip the training images\n",
    "])\n",
    "\n",
    "# Define data transforms for validation batches\n",
    "val_tfm = T.ToTensor()\n",
    "\n",
    "# Define datasets\n",
    "dataset_train = PennFudanDataset('data/PennFudanPed/', train_tfm)\n",
    "dataset_val = PennFudanDataset('data/PennFudanPed/', val_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5dGaIezze3y",
    "outputId": "5cf908c4-a3c6-4c7b-b6f5-7633e8b43594"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from lib.detection.utils import collate_fn\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    dataset_val, batch_size=2, shuffle=False, num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5yvZUprj4ZN"
   },
   "source": [
    "Now let's instantiate the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoenkCj18C4h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and product\n",
    "num_classes = 2\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = StepLR(optimizer,\n",
    "                      step_size=3,\n",
    "                      gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start TensorBoard for logging\n",
    "\n",
    "To start TensorBoard on VSC OnDemand, go to [the dashboard](https://ondemand.hpc.kuleuven.be/pun/sys/dashboard/) and click on \"TensorBoard\". Use the following settings:\n",
    "\n",
    "- Number of cores: 1\n",
    "- Account: lp_edu_maibi_anndl\n",
    "- Partition: interactive\n",
    "- Project/Log folder: maibi_cv/3_detection/runs\n",
    "- Number of hours: 4\n",
    "- Number of gpu's: 0\n",
    "\n",
    "Leave the other settings at their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAd56lt4kDxc"
   },
   "source": [
    "And now let's train the model for a couple of epochs, evaluating at the end of every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "at-h4OWK0aoc",
    "outputId": "ab6753bc-6251-42e5-d65b-32efec93c61e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lib.detection.engine import train_one_epoch, evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train,\n",
    "                    device, epoch, writer=writer)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the validation dataset\n",
    "    evaluate(model, data_loader_val, device, epoch, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6mYGFLxkO8F"
   },
   "source": [
    "Now that training has finished, let's have a look at what it actually predicts in a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHwIdxH76uPj"
   },
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_val[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmN602iKsuey"
   },
   "source": [
    "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
    "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels` and `scores` as fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lkmb3qUu6zw3",
    "outputId": "7e876402-3a0f-4696-a13e-6ddcabb5fb77"
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwT21rzotFbH"
   },
   "source": [
    "Let's inspect the image and the predicted detection boxes.\n",
    "\n",
    "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format. Next, we iterate over the predicted boxes and draw them on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "im = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for box in prediction[0]['boxes'].cpu().numpy():\n",
    "    draw.rectangle(box, width=5)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "torchvision_finetuning_instance_segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "maibi_cv",
   "language": "python",
   "name": "maibi_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
