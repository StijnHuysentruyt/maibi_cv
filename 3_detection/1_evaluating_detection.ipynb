{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5eed07",
   "metadata": {},
   "source": [
    "We have already learned about [evaluation metrics for classification](../2_classification/2_evaluation_metrics.ipynb). In object detection, we also perform some kind of classification. More specifically, a detector returns a set of bounding boxes with corresponding class labels. As such, the model has classified the image regions that correspond to the bounding boxes.\n",
    "\n",
    "Just like any classification problem, the label that is assigned to a certain image region can either be correct or incorrect. Let's say the model assigns a certain class label *A* to an image region. Then, there are two options\n",
    "\n",
    "1. **True positive** (TP) of label *A*: the region is predicted to have label *A* and indeed has label *A*\n",
    "2. **False positive** (FP) of label *A*: the region is predicted to have label *A*, but *does not* have label *A*\n",
    "\n",
    "Of course, there will be many image regions that **do not receive a certain label** from the detector. Let's say the model did not assign label *A* to a certain image region. Then, again, there are two possibilities:\n",
    "\n",
    "1. **True negative** (TN) of label *A*: the region is predicted to *not* have label *A* and indeed does not have label *A*\n",
    "2. **False negative** (FN) of label *A*: the region is predicted to *not* have label *A*, but *does* have label *A*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419006b",
   "metadata": {},
   "source": [
    "To evaluate the predictions, we compare them with the **ground-truth bounding boxes**. However, **predictions will almost never exactly overlap** with a ground-truth bounding box. Of course, this does not mean that all predicted bounding boxes are *bad* and should be counted as false positives. Instead, we use the **Intersection over Union** (IoU) to define when a predicted bounding box is a TP, FP or FN for a certain class.\n",
    "\n",
    "We can choose this IoU ourselves. Let's say we give it a value $\\alpha$. Then a TP, FP and FN of a class label *A* at this IoU level $\\alpha$ is defined as follows:\n",
    "\n",
    "- **True positive at $\\alpha$** ($\\text{TP}_\\alpha$) of label *A*: the predicted bounding box has label *A* and has an IoU $\\ge \\alpha$ with a ground truth bounding box that has label *A*\n",
    "- **False positive at $\\alpha$** ($\\text{FP}_\\alpha$) of label *A*: the predicted bounding box has label *A*, but does not have an IoU $\\ge \\alpha$ with a ground truth bounding box that has label *A*\n",
    "- **False negative at $\\alpha$** ($\\text{FN}_\\alpha$) of label *A*: a ground truth bounding box that has label *A*, without there being any prediction of label *A* that has an IoU $\\ge \\alpha$ with it\n",
    "\n",
    "Defining *True negatives at $\\alpha$* is not necessary, since we do not use it to calculate precision and recall. This makes sense, because the amount of regions in an image that *do not* belong to a certain class is huge and rather uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99359c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def get_tiled_regions(region_size, n_regions_per_dim):\n",
    "    \"\"\"\n",
    "    Return the (x1, y1, x2, y2) coordinates of a number of tiled regions of the given size.\n",
    "    \n",
    "    If num_regions is not a square, the next square number of regions will be returned.\n",
    "    \"\"\"\n",
    "    region_starts = np.array(range(0, n_regions_per_dim * region_size,\n",
    "                                   region_size))\n",
    "    region_ends = region_starts + region_size\n",
    "\n",
    "    region_x1y1 = np.array(list(product(region_starts, region_starts)))\n",
    "    region_x2y2 = np.array(list(product(region_ends, region_ends)))\n",
    "\n",
    "    return np.hstack([region_x1y1, region_x2y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be60e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, uniform\n",
    "\n",
    "\n",
    "def get_random_box_in_region(region, min_rel_size=0.3,\n",
    "                             min_ar=0.5, max_ar=2.0):\n",
    "    region_width = region[2] - region[0]\n",
    "    region_height = region[3] - region[1]\n",
    "\n",
    "    w = randint(int(region_width*min_rel_size), region_width)\n",
    "    ar = uniform(min_ar, max_ar)\n",
    "    h = min(int(w/ar), region_height - 1)\n",
    "\n",
    "    x1 = randint(region[0], region[2] - w)\n",
    "    y1 = randint(region[1], region[3] - h)\n",
    "    x2 = min(x1 + w, region[2])\n",
    "    y2 = min(y1 + h, region[3])\n",
    "\n",
    "    return np.array([\n",
    "        x1, y1, x2, y2\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a88751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import sample, randint, uniform\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def generate_pseudo_boxes(\n",
    "    img_size=500,\n",
    "    num_boxes=25,\n",
    "    n_labels=3,\n",
    "):\n",
    "    n_boxes_per_dim = math.ceil(math.sqrt(num_boxes))\n",
    "    region_size = img_size // n_boxes_per_dim\n",
    "\n",
    "    regions = get_tiled_regions(region_size, n_boxes_per_dim)\n",
    "    regions = np.array(sample(list(regions), num_boxes))\n",
    "\n",
    "    boxes = np.vstack([\n",
    "        get_random_box_in_region(region, min_ar=1.0, max_ar=1.0)\n",
    "        for region in regions\n",
    "    ])\n",
    "\n",
    "    labels = np.random.randint(0, n_labels, num_boxes)\n",
    "    \n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a72519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x7F17724C4940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = 500\n",
    "num_boxes = 25\n",
    "n_labels = 3\n",
    "\n",
    "boxes, labels = generate_pseudo_boxes(img_size, num_boxes, n_labels)\n",
    "\n",
    "img = np.ones((img_size, img_size, 3), np.uint8)*240\n",
    "\n",
    "im = Image.fromarray(img)\n",
    "\n",
    "\n",
    "label_colors = ['orange', 'blue', 'green']\n",
    "\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for box, label in zip(boxes, labels):\n",
    "    color = label_colors[label]\n",
    "    draw.rectangle(tuple(box), width=5, outline=color)\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515fc638",
   "metadata": {},
   "source": [
    "Now that we have defined how we to calculate $\\text{TP}_\\alpha$, $\\text{FP}_\\alpha$ and $\\text{FN}_\\alpha$ for a certain class label at a give IoU level $\\alpha$, we can calculate the precision and recall at that IoU level:\n",
    "\n",
    "$$\\text{Precision}_\\alpha = \\frac{\\text{TP}_\\alpha}{\\text{TP}_\\alpha + \\text{FP}_\\alpha}$$\n",
    "\n",
    "$$\\text{Recall}_\\alpha = \\frac{\\text{TP}_\\alpha}{\\text{TP}_\\alpha + \\text{FN}_\\alpha}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208334d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b837b228",
   "metadata": {},
   "source": [
    "The detector will return a score that indicates how convinced it is of its prediction. As with classification, we can threshold this score to improve our precision. When we plot the precision and recall for all possible threshold values, we obtain a PR-curve for the chosen IoU level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818d9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9256698",
   "metadata": {},
   "source": [
    "To summarize the PR-curve at the chosen IoU level, we can again compute the **average precision**, which is the integral of the PR-curve.\n",
    "\n",
    "$$\n",
    "\\text{AP}_\\alpha = \\sum_{t} P_\\alpha(t)\\cdot (R_{\\alpha}(t) - R_\\alpha(t-1))\n",
    "$$\n",
    "\n",
    "with $P_\\alpha(t)$ and $R_\\alpha(t)$ the precision, resp. recall, at threshold $t$ and IoU level $\\alpha$ ($R_\\alpha(0)$ is defined as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a02d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5614e5",
   "metadata": {},
   "source": [
    "## Mean Average Precision\n",
    "\n",
    "Up until now, we have calculated each metric *for a single class*. The **mean Average Precision at $\\alpha$** ($\\text{mAP}_\\alpha$) is the mean of the $\\text{AP}_\\alpha$ of each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4c34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1611d26d",
   "metadata": {},
   "source": [
    "## COCO AP\n",
    "\n",
    "We can calculate $\\text{mAP}_\\alpha$ for different values of $\\alpha$ (the IoU level). In the literature, **COCO AP** is frequently used as a metric. It is computed as the average of $\\text{mAP}_{0.50}$, $\\text{mAP}_{0.55}$, $\\text{mAP}_{0.60}$, ..., $\\text{mAP}_{0.95}$. It is written as $\\text{mAP}_{[.5:.05:.95]}$ or $\\text{AP}_{\\text{COCO}}$ or even just $\\text{AP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a4ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
