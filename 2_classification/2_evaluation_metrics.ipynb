{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "In this lab you will learn different metrics, how they are computed and how they can be analysed.\n",
    "\n",
    "We will apply each of the metrics to classification with the histogram comparison method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.soda_dataset import get_gallery_and_queries\n",
    "from lib.histogram_comparison import match_opp_color_hist\n",
    "\n",
    "# Create our gallery and queries\n",
    "gallery, queries, gallery_labels, true_labels = get_gallery_and_queries()\n",
    "\n",
    "# Compute the similarity matrix\n",
    "sim_mat = match_opp_color_hist(gallery, queries)\n",
    "\n",
    "# Find the best match index for each query\n",
    "match_idxs = sim_mat.argmax(axis=1)\n",
    "\n",
    "# Map the match indices to labels\n",
    "pred_labels = gallery_labels[match_idxs]\n",
    "\n",
    "# Map the match indices to similarity scores\n",
    "pred_scores = sim_mat[np.arange(len(sim_mat)), match_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy** determines what fraction of all predictions were correct. It is defined as:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Number of predictions}}$$\n",
    "\n",
    "A prediction is correct when the true label and the predicted label are the same. The accuracy is often used to assess the performance of a classification model.\n",
    "\n",
    "However, the accuracy metric ignores all information about the specific class labels, it only checks whether or not the predicted is the same as the true label. For example, we do not know if there are any classes that are misclassified very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_true=true_labels,\n",
    "                     y_pred=pred_labels)\n",
    "print('The accuracy is {acc:.2f}%'.format(acc=acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The accuracy does not say anything about the performance on specific classes. A **confusion matrix**, on the other hand, summarizes correct and wrong classifications nicely for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from lib.plots import plot_conf_mat\n",
    "\n",
    "labels = np.unique(true_labels)\n",
    "\n",
    "cmat = confusion_matrix(y_true=true_labels,\n",
    "                        y_pred=pred_labels,\n",
    "                        labels=labels)\n",
    "\n",
    "plot_conf_mat(cmat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **rows correspond to the true label**, while the **columns correspond to the predicted label**. As such, the diagonal contains all correct predictions.\n",
    "\n",
    "For a given label *A*, there are two ways for a query to be correctly classified and two ways for it to be wrongly classified:\n",
    "\n",
    "Correctly classified:\n",
    "\n",
    "1. **True positive** (TP) of label *A*: the query is predicted to have label *A* and indeed has label *A*\n",
    "2. **True negative** (TN) of label *A*: the query is predicted to *not* have label *A* and indeed does not have label *A*\n",
    "\n",
    "Wrongly classified:\n",
    "\n",
    "1. **False positive** (FP) of label *A*: the query is predicted to have label *A*, but *does not* have label *A*\n",
    "2. **False negative** (FN) of label *A*: the query is predicted to *not* have label *A*, but *does* have label *A*\n",
    "\n",
    "For each label in the dataset, we can compute the number of TP, TN, FP and FN. This will give us a much more detailed idea of how well our classifier works for each label in the dataset.\n",
    "\n",
    "The number of TPs, TNs, FPs and FNs of each label in the dataset, can be computed from the confusion matrix. Let's say that label *A* corresponds to row *i* and column *i* of the confusion matrix. Then check for yourself that:\n",
    "\n",
    "* The number of **TPs** of label *A* is the value at **cell $(i, i)$** of the confusion matrix\n",
    "* The number of **TNs** of label *A* is the sum of the cells that are **not in row $i$, nor in column $i$**\n",
    "* The number of **FPs** of label *A* is the sum of the cells in **column $i$, excluding cell $(i, i)$**\n",
    "* The number of **FNs** of label *A* is the sum of the cells in **row $i$, excluding cell $(i, i)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.evaluation_metrics import calc_tp_tn_fp_fn\n",
    "\n",
    "print(f'{\"Label\":<15}|{\"TP\":^5}|{\"TN\":^5}|{\"FP\":^5}|{\"FN\":^5}')\n",
    "print(f'{\"-\"*15}|{\"-\"*5}|{\"-\"*5}|{\"-\"*5}|{\"-\"*5}')\n",
    "\n",
    "for label in labels:\n",
    "    tp, tn, fp, fn = calc_tp_tn_fp_fn(label, cmat, labels)\n",
    "    print(f'{label:<15}|{tp:^5}|{tn:^5}|{fp:^5}|{fn:^5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall\n",
    "\n",
    "The number of TPs, FPs, FNs and TNs of a certain label are often summarized with the **precision** and **recall**. These are defined as:\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "where $\\text{TP}$ is the number of true positives and $\\text{FP}$ is the number of false positives of the label. For a given class *A*, the precision can be interpreted as **the fraction of queries that truly belong to class *A* when our model said they belonged to class *A***.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "where $\\text{TP}$ is the number of true positives and $\\text{FN}$ is the number of false negatives.  For a given class *A*, the recall can be interpreted as **the fraction of all queries with class *A* that that our classifier was able to retrieve**.\n",
    "\n",
    "Depending on the application, a high precision might be more important than a high recall or vice versa.\n",
    "\n",
    "Since precision and recall are computed with TPs, FPs and FNs, they can also be directly computed from the confusion matrix. Let's say that label *A* corresponds to row *i* and column *i* of the confusion matrix. Then check for yourself that:\n",
    "\n",
    "* **Precision** is the value at **cell $(i, i)$** divided by the sum of the cells in **column $i$**\n",
    "* **Recall** is the value at **cell $(i, i)$** divided by the sum of the cells in **row $i$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.evaluation_metrics import calc_precision, calc_recall\n",
    "\n",
    "precisions = [\n",
    "    calc_precision(label, cmat, labels)\n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "recalls = [\n",
    "    calc_recall(label, cmat, labels)\n",
    "    for label in labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_LABELS = len(labels)\n",
    "x = np.arange(N_LABELS)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(zorder=0)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Plot precisions\n",
    "ax.bar(x=x - width/2, height=precisions, label='Precision',\n",
    "       width=width, zorder=3)\n",
    "\n",
    "# Plot recalls\n",
    "ax.bar(x=x + width/2, height=recalls,label='Recall',\n",
    "       width=width, zorder=3)\n",
    "\n",
    "# Add legend\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR-curve\n",
    "\n",
    "Up until now, we evaluate our model by looking at which label matches best with a query, according to our model. We do not take into account the similarity score that is assigned by the model. However, by thresholding that similarity value, we might increase the precision of the model (albeit at the cost of a lower recall). For each such thresholding value, we can recompute precision and recall. When plotting all these precision-recall pairs, we obtain the **precision-recall curve**.\n",
    "\n",
    "A PR curve shows how the precision and recall vary for different threshold values. When the curve reaches the top right point of the graph, there exists a threshold for which the classifier works perfectly for the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.evaluation_metrics import calc_pr_curve\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for label in gallery_labels:\n",
    "    p, r, _ = calc_pr_curve(label, sim_mat, gallery_labels, true_labels)\n",
    "    \n",
    "    # Add (1.0, 0.0) for nicer visualization\n",
    "    p = np.array([0, *p])\n",
    "    r = np.array([1, *r])\n",
    "\n",
    "    ax.plot(r, p, label=label)\n",
    "    ax.set_xlabel('Precision')\n",
    "    ax.set_ylabel('Recall')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision\n",
    "\n",
    "To summarize the PR-curve, we can compute the **average precision**, which is the integral of the PR-curve.\n",
    "\n",
    "$$\n",
    "\\text{AP} = \\sum_{t} P_{t}\\cdot (R_{t} - R_{t-1})\n",
    "$$\n",
    "\n",
    "with $P_t$ and $R_{t}$ the precision, resp. recall, at threshold $t$ ($R_0$ is defined as 1).\n",
    "\n",
    "Intuitively, the AP measures how well our algorithm sorted the query items from most to least similar to a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.evaluation_metrics import calc_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(zorder=0)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "for label in labels:\n",
    "    ap = calc_ap(label, sim_mat, gallery_labels, true_labels)\n",
    "    ax.bar(label, ap, zorder=3)\n",
    "    ax.set_ylabel('Average Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
